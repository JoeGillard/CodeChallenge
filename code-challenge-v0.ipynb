{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29acb687-4b2f-48af-af71-1de5a5a67c7e",
   "metadata": {},
   "source": [
    "# Code challenge: self-supervised learning and embeddings generation\n",
    "To address this challenge, we will follow the SimCLR approach of Chen et al. (2020)...\n",
    "\n",
    "## Import libraries and modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c4a1379d-bee8-4058-8920-ee495c453bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch # load pytorch for machine learning\n",
    "import torch.nn as nn # load module for neural networks\n",
    "import torchvision.transforms as tvtran # load module for transformations to transform / augment data \n",
    "import torchvision.datasets as tvdat # load module for handling datasets\n",
    "import torchvision.models as tvmod\n",
    "from torchvision.models import resnet18, ResNet18_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27680151-1946-4adc-885e-a6263b17f78e",
   "metadata": {},
   "source": [
    "## Dataset loading\n",
    "We begin by loading the dataset of pet images from the root folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8e9e878a-cc5f-41ea-9abd-4966867753a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_raw = tvdat.ImageFolder('../CodeChallenge/data/', transform=None) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d64ea1-12c3-4948-926a-643738c4eab8",
   "metadata": {},
   "source": [
    "## Dataset augmentation\n",
    "Next we specify a set of transformations to aid contrastive learning, based on recommendations in the SimCLR paper [Chen et al. (2020)](https://arxiv.org/pdf/2002.05709):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b1b5a24f-0f18-4729-9e68-8e1795dd1ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = tvtran.Compose([\n",
    "    # spatial transformations\n",
    "    tvtran.RandomResizedCrop(size=224), # random crop, resized to 224x224\n",
    "    tvtran.RandomHorizontalFlip(p=0.5), # flip horizontally 50% of the time\n",
    "    # colour distortion\n",
    "    tvtran.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.2), # illustrative parameters\n",
    "    tvtran.RandomGrayscale(p=0.2), # convert to greyscale with probability 20%\n",
    "    tvtran.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)), # illustrative parameters\n",
    "    # convert to tensor\n",
    "    tvtran.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63584d23-7a7e-48c8-80f5-a132bda75660",
   "metadata": {},
   "source": [
    "The SimCLR approach requires two augmented versions of each image. Define a class for generating a customised dataset, using the base class `torch.utils.data.Dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "845f4c6d-2ee2-4398-b05f-ef1327331ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLR_custom_dat(torch.utils.data.Dataset):\n",
    "    def __init__(self, raw_dataset, transform):\n",
    "        self.raw_dataset = raw_dataset\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.raw_dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.raw_dataset[idx] # image and label from ImageFolder dataset\n",
    "        # apply two stochastic augmentations to the same image\n",
    "        x1 = self.transform(img)\n",
    "        x2 = self.transform(img)\n",
    "        return x1, x2, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e24ea60-bc3d-48eb-9086-749062c4ded6",
   "metadata": {},
   "source": [
    "Now, we can generate our customised dataset of augmented (and challenging) images, providing positive pairs for contrastive learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "abd466bc-9d1a-49e2-9944-ce7d70cf896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_simclr = SimCLR_custom_dat(x_raw, transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df41c0e-7468-4560-85d8-ed6aab110572",
   "metadata": {},
   "source": [
    "## Data loader\n",
    "Create a data loader for inputting images into the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "91cf5eef-2a79-46f0-93fc-4c124a4add24",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 # this is arbitrary; may be tweaked later\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(x_simclr, batch_size=batch_size, shuffle=True, drop_last=True) # drop_last=True drops incomplete batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8019d3ce-b4d9-4470-b399-9bb42abe0427",
   "metadata": {},
   "source": [
    "## Base encoder\n",
    "The framework specified in Chen et al. (2020) consists of a base encoder framework to create representations of the positive pairs (denoted `f` in the paper, see Fig. 2) and a projection head (denoted `g`). \n",
    "\n",
    "For the base encoder, we will use ResNet18 together with a set of pre-trained weights. This model is selected as a relatively small network, to show proof-of-concept without requiring large compute resources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d66c7c58-1cb8-4fa8-a9ef-8f96b99fde72",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1) # load ResNet18 as the base encoder, together with pre-trained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3095ca97-c4f0-46de-8009-5329f92e574a",
   "metadata": {},
   "source": [
    "ResNet18 has 1000 classes of animals in its output layer, but we are interested in just 3 classes (chinchilla, rabbit, hamster). Therefore, we truncate the base model to remove the final layer. The output of this truncated model will be that of the penultimate layer (average pooling), a 512-dimensional feature vector for each image: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "34afeaac-b297-4490-ad73-4215b71b0959",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_trunc = nn.Sequential(*list(f.children())[:-1]) # remove final classification layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a69e921-9965-4c93-b7d4-267506974191",
   "metadata": {},
   "source": [
    "## Projection head\n",
    "The projection head `g` is a small neural network that maps the representations generated by `f_trunc` to the space where contrastive lost is applied. \n",
    "\n",
    "Following Chen et al. (2020), we define `g` to be a multi-layer perceptron with one hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "55d33e38-6497-4d38-965a-7291be55f899",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nn.Sequential(\n",
    "    nn.Linear(512, 512), # hidden layer\n",
    "    nn.ReLU(), # activation function\n",
    "    nn.Linear(512, 128) # output layer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c71ff9-7095-4aee-b378-4fb7f95f6599",
   "metadata": {},
   "source": [
    "## Combined model\n",
    "Next, we use the pytorch base class `torch.nn.Module` to define our custom neural network architecture, which will combine the encoder `f_trunc` with the projection head `g`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "896cb6cc-2cba-4b7e-a8ce-c38df51629f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, ff, gg):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.encoder = ff\n",
    "        self.projection_head = gg\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)           # representation of x generated by f_trunc; shape: [batch, 512, 1, 1] for ResNet18\n",
    "        h = h.view(h.size(0), -1)     # flatten to shape: [batch, 512]\n",
    "        z = self.projection_head(h)         # apply projection head, to project to 128-dim; z will be used in the constrative loss calculation\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e200bb-9958-4748-8ced-c4a4ec422177",
   "metadata": {},
   "source": [
    "Create an instance of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "511f2aff-81e5-4835-8a36-4b8c082fbc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomModel(f_trunc, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e165f9-de3e-4c75-8897-db9889298ea6",
   "metadata": {},
   "source": [
    "## Training parameters\n",
    "Here, we specify the device on which the training will occur (CPU will be used here for demonstration), and also the optimizer to be used in training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6de77b5e-8c8d-4dec-b1d6-841ad4c21ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move model to CPU\n",
    "model = model.to('cpu')\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4) # use Adam optimizer with low learning rate (lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b8ea7b-aa03-4775-ae13-abf8040ecd41",
   "metadata": {},
   "source": [
    "## Self-supervised training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703f13c0-27bc-4be9-a123-5f8d54e6e5de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
